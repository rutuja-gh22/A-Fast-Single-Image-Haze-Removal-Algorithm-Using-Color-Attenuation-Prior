# -*- coding: utf-8 -*-
"""SEM5_DIP_CaseStudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xWSzvrgwIYApKYgU-VqUpvJP-V5gmj3

**A Fast Single Image Haze Removal Algorithm
 Using Color Attenuation Prior**

 Implemented by UCE2022469 & UCE2022625
"""

!pip install opencv-python-headless

"""Set Up the Environment"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load image
img = cv2.imread('/foggy.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for display

# Display the image
plt.imshow(img)
plt.axis('off')
plt.title("Original Hazy Image")
plt.show()

"""Color Attenuation Prior for Depth Estimation"""

hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
brightness = hsv[:, :, 2] / 255.0  # Normalized brightness (V channel)
saturation = hsv[:, :, 1] / 255.0  # Normalized saturation (S channel)

theta_0 = 0.121779
theta_1 = 0.959710
theta_2 = -0.780245

# Depth estimation
depth = theta_0 + theta_1 * brightness + theta_2 * saturation
depth = (depth - depth.min()) / (depth.max() - depth.min())  # Normalize depth

plt.imshow(depth, cmap='gray')
plt.axis('off')
plt.title("Estimated Depth Map")
plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Assuming 'dehazed' is the image obtained from the previous dehazing process.

# Convert the dehazed image to grayscale, as depth estimation often works better in a single channel.
gray_dehazed = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

# Normalize the grayscale image to enhance depth perception.
# Normalize to the range [0, 1] for better depth map scaling
normalized_depth = cv2.normalize(gray_dehazed.astype('float32'), None, 0.0, 1.0, cv2.NORM_MINMAX)

# Convert the normalized depth to a colormap to visualize it like a heatmap
depth_map = cv2.applyColorMap((normalized_depth * 255).astype('uint8'), cv2.COLORMAP_JET)

# Display the depth map
plt.imshow(depth_map)
plt.axis('off')
plt.title("Depth Map")
plt.show()

"""Refine Depth Map with Guided Filtering"""

def guided_filter(I, p, r, eps):
    mean_I = cv2.boxFilter(I, cv2.CV_64F, (r, r))
    mean_p = cv2.boxFilter(p, cv2.CV_64F, (r, r))
    mean_Ip = cv2.boxFilter(I * p, cv2.CV_64F, (r, r))
    cov_Ip = mean_Ip - mean_I * mean_p

    mean_II = cv2.boxFilter(I * I, cv2.CV_64F, (r, r))
    var_I = mean_II - mean_I * mean_I

    a = cov_Ip / (var_I + eps)
    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, cv2.CV_64F, (r, r))
    mean_b = cv2.boxFilter(b, cv2.CV_64F, (r, r))

    q = mean_a * I + mean_b
    return q

I = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0  # Guidance image
refined_depth = guided_filter(I, depth, r=15, eps=0.001)

plt.imshow(refined_depth, cmap='gray')
plt.axis('off')
plt.title("Refined Depth Map with Guided Filtering")
plt.show()

"""Recover Haze-Free Image"""

# Estimate transmission map
beta = 1.0
transmission = np.exp(-beta * refined_depth)

# Estimate atmospheric light
bright_pixels = np.percentile(refined_depth, 99.9)
atmosphere = img[refined_depth >= bright_pixels].mean(axis=0)

# Estimate transmission map
beta = 1.0
transmission = np.exp(-beta * refined_depth)

# Estimate atmospheric light
bright_pixels = np.percentile(refined_depth, 99.9)
atmosphere = img[refined_depth >= bright_pixels].mean(axis=0)

# Reshape transmission to have 3 channels to match img shape
transmission = transmission[..., np.newaxis]  # Add a new axis to make it (4080, 3072, 1)
transmission = np.repeat(transmission, 3, axis=2) # Repeat along the new axis to get (4080, 3072, 3)


dehazed = (img - atmosphere) / np.clip(transmission, 0.1, 0.9) + atmosphere
dehazed = np.clip(dehazed, 0, 255).astype(np.uint8)

# Display the dehazed image
plt.imshow(dehazed)
plt.axis('off')
plt.title("Dehazed Image")
plt.show()

"""Apply Contrast Enhancement"""

# Contrast stretching
min_val, max_val = dehazed.min(), dehazed.max()
contrast_stretched = ((dehazed - min_val) / (max_val - min_val) * 255).astype(np.uint8)

# Display contrast-stretched image
plt.figure(figsize=(12, 5))  # Create a single figure with desired size
plt.imshow(contrast_stretched)
plt.axis('off')
plt.title("Contrast-Stretched Image")
plt.show()

# Image Compression Techniques
# Save the dehazed image with JPEG compression
cv2.imwrite('/content/dehazed_image.jpg', dehazed, [int(cv2.IMWRITE_JPEG_QUALITY), 90])

# Step 2: Multi-Scale Edge Detection
edges_fine = cv2.Canny(dehazed, 50, 150)  # Fine details
edges_coarse = cv2.Canny(dehazed, 100, 250)  # Coarse features
edges_combined = cv2.addWeighted(edges_fine, 0.5, edges_coarse, 0.5, 0)

# Display edge detection results
plt.imshow(edges_combined, cmap='gray')
plt.axis('off')
plt.title("Combined Edge Detection")
plt.show()

# Step 3: Edge-Enhanced Visualization
edges_rgb = cv2.cvtColor(edges_combined, cv2.COLOR_GRAY2RGB)
highlighted_edges = cv2.addWeighted(dehazed, 0.8, edges_rgb, 0.2, 0)

# Display edge-enhanced image
plt.imshow(highlighted_edges)
plt.axis('off')
plt.title("Edge-Enhanced Image")
plt.show()

# Step 4: ROI Segmentation
# Morphological operations
kernel = np.ones((5, 5), np.uint8)
refined_edges = cv2.morphologyEx(edges_combined, cv2.MORPH_CLOSE, kernel)

# Mask for segmentation
roi_mask = cv2.threshold(refined_edges, 127, 255, cv2.THRESH_BINARY)[1]
segmented_image = cv2.bitwise_and(dehazed, dehazed, mask=roi_mask)

# Display segmented image
plt.imshow(segmented_image)
plt.axis('off')
plt.title("Segmented Regions")
plt.show()

# Step 5: Final Visualization - Progressive Results
fig, ax = plt.subplots(2, 3, figsize=(18, 10))

# Original Image
ax[0, 0].imshow(img)
ax[0, 0].set_title("Original Hazy Image")
ax[0, 0].axis('off')

# Estimated Depth Map
ax[0, 1].imshow(refined_depth, cmap='gray')
ax[0, 1].set_title("Refined Depth Map")
ax[0, 1].axis('off')

# Dehazed Image
ax[0, 2].imshow(dehazed)
ax[0, 2].set_title("Dehazed Image")
ax[0, 2].axis('off')

# Combined Edge Detection
ax[1, 0].imshow(edges_combined, cmap='gray')
ax[1, 0].set_title("Edge Detection (Fine + Coarse)")
ax[1, 0].axis('off')

# Edge-Enhanced Visualization
ax[1, 1].imshow(highlighted_edges)
ax[1, 1].set_title("Edge-Enhanced Image")
ax[1, 1].axis('off')

# Segmented Regions (ROI)
ax[1, 2].imshow(segmented_image)
ax[1, 2].set_title("Segmented Regions (ROI)")
ax[1, 2].axis('off')

plt.tight_layout()
plt.show()

def combine_with_edges_natural(dehazed, edges):
    # Normalize edges for consistent scaling
    edges_normalized = cv2.normalize(edges, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

    # Convert edges to light gray (so they blend more naturally with the image)
    edges_colored = cv2.cvtColor(edges_normalized, cv2.COLOR_GRAY2BGR)
    edges_colored = cv2.addWeighted(edges_colored, 0.5, np.zeros_like(edges_colored), 0.5, 0)  # Light gray edges

    # Overlay the edges on top of the dehazed image
    final_composite = cv2.addWeighted(dehazed, 0.8, edges_colored, 0.2, 0)

    return final_composite

# Combine dehazed image with edge detection
final_composite_with_edges = combine_with_edges_natural(dehazed, edges_combined)

# Visualize the Final Composite Image with Edge Detection
plt.figure(figsize=(10, 10))
plt.imshow(final_composite_with_edges)
plt.axis('off')
plt.title("Dehazed Image with Edge Detection")
plt.show()